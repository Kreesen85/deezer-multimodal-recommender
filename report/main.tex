\documentclass[]{ceurart}

\sloppy

\usepackage{listings}
\lstset{breaklines=true}
\usepackage{booktabs}
\usepackage{amsmath}

\begin{document}

\copyrightyear{2026}
\copyrightclause{Copyright for this paper by its authors.
  Use permitted under Creative Commons License Attribution 4.0
  International (CC BY 4.0).}

\conference{Recommender Systems Course Project, February 2026}

\title{Predicting Music Listening Behaviour on Deezer: From Competition Pipelines to Production Recommender Systems}

\author[1]{Vimerlin Govender}[%
email=vimerlin.govender@stud.hslu.ch,
]
\author[1]{Meleknur \"Ozg\"u}[%
email=meleknur.oezgue@stud.hslu.ch,
]
\author[1]{Xinmeng Song}[%
email=xinmeng.song@stud.hslu.ch,
]
\address[1]{Lucerne University of Applied Sciences and Arts (HSLU)}

%%
\begin{abstract}
We address the Deezer skip prediction task from the 2017 Data Science Game, predicting whether a user will listen to or skip the first track recommended by Deezer's Flow feature. Our approach compares feature-based classification, implicit matrix factorisation (ALS), and a hybrid combining MF embeddings with gradient-boosted trees. Crucially, we compare random and temporal validation: a na\"ive random split yields 0.758 AUC, but a temporal split---which mirrors the competition's test distribution---produces 0.686 AUC, matching the competition winner. An ablation study reveals that user--item affinity features, while dominant on random splits, \emph{degrade} temporal performance by 5 percentage points. Pure ALS achieves only 0.520, and MF embeddings do not improve the hybrid model, demonstrating that skip prediction is fundamentally a contextual classification problem rather than a preference estimation task. We contrast this solution with a production architecture and analyse where competition and production approaches overlap and diverge.
\end{abstract}

\begin{keywords}
  recommender systems \sep
  music recommendation \sep
  skip prediction \sep
  collaborative filtering \sep
  gradient boosting \sep
  feature engineering
\end{keywords}

\maketitle

%% ============================================================================
\section{Introduction}
\label{sec:intro}

Music streaming platforms such as Deezer serve hundreds of millions of tracks to users with diverse tastes. Deezer's \emph{Flow} feature provides a personalised, infinite stream of music---a mix of familiar favourites and new discoveries. Predicting whether a user will engage with (listen to) or reject (skip) a Flow recommendation is central to improving this experience~\cite{schedl2018current}.

The Deezer Data Science Game 2017 (DSG17) formalises this as binary classification: given a user's one-month listening history, predict whether they will listen to or skip the first track Flow recommends~\cite{deezer2017dsg}. A track is ``listened'' if the user hears more than 30 seconds (\texttt{is\_listened}~=~1); pressing skip before 30 seconds yields \texttt{is\_listened}~=~0. The winning team achieved a test AUC of 0.686.

We pursue three objectives. First, we develop a competition-focused solution (\textbf{Q1}, Section~\ref{sec:competition}). Second, we propose a production-grade recommendation system (\textbf{Q2}, Section~\ref{sec:production}). Third, we critically compare these perspectives (\textbf{Q3}, Section~\ref{sec:comparison}). Our key contribution is demonstrating---through temporal validation and ablation analysis---that evaluation methodology fundamentally determines reported performance, and that features which appear dominant under convenient splits can actively harm generalisation.

%% ============================================================================
\section{Dataset and Exploratory Analysis}
\label{sec:data}

\subsection{Dataset Description}

The training set contains 7,558,834 interactions from 19,914 users across 451,867 tracks, representing one month of listening history. Each record includes 15 features: user and item identifiers (\texttt{user\_id}, \texttt{media\_id}, \texttt{artist\_id}, \texttt{album\_id}, \texttt{genre\_id}), a Unix timestamp (\texttt{ts\_listen}), track release date, song duration (\texttt{media\_duration}), platform and context metadata (\texttt{platform\_name}, \texttt{platform\_family}, \texttt{listen\_type}, \texttt{user\_gender}, \texttt{user\_age}, \texttt{context\_type}), and the binary target \texttt{is\_listened}. The dataset has 100\% completeness (no missing values) and zero duplicate rows.

The test set contains 19,918 rows---one per user---each representing the \emph{first track Flow recommended}. This structural difference is critical: the training data captures general listening behaviour (many tracks per user), while the test set captures a single, algorithmically curated recommendation event per user.

\subsection{Target Distribution and User Segmentation}

The target exhibits moderate class imbalance: 68.4\% listens vs.\ 31.6\% skips (ratio 0.46:1). We identified five distinct user segments from per-user skip rates (Table~\ref{tab:segments}), ranging from users who almost never skip (6.8\%) to frequent skippers (26.9\%).

\begin{table}[t]
  \caption{User engagement segments based on training set skip rates}
  \label{tab:segments}
  \begin{tabular}{lrrr}
    \toprule
    Segment & Users (\%) & Skip Rate & Avg.\ Age \\
    \midrule
    Never Skip ($<$1\%)     &  6.8\% & $<$1\%    & 25.0 \\
    Rarely Skip (1--10\%)   & 12.5\% & 1--10\%   & 24.8 \\
    Occasional (10--25\%)   & 23.1\% & 10--25\%  & 24.1 \\
    Moderate (25--50\%)     & 30.7\% & 25--50\%  & 23.6 \\
    Frequent ($>$50\%)      & 26.9\% & $>$50\%   & 23.3 \\
    \bottomrule
  \end{tabular}
\end{table}

Figure~\ref{fig:skip_behavior} visualises this segmentation. Engaged users ($<$10\% skip rate) are significantly older (25.0 vs.\ 23.3 years, $p < 0.001$) and have more sessions (110 vs.\ 91, $p < 0.001$) than frequent skippers, but genre diversity does not differ ($p = 0.545$)---selective users are discerning across similar musical breadth.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/user_skip_behavior}
  \caption{User skip behaviour analysis. Top left: segment sizes. Top right: skip rate distribution. Bottom: age and session count comparison between engaged and frequent skippers.}
  \label{fig:skip_behavior}
\end{figure}

\subsection{Temporal Patterns and Data Quality}

Listen rates peak at 2--4\,AM (0.77) when only dedicated listeners are active, and drop to 0.63 around 10--11\,PM during casual browsing (Figure~\ref{fig:temporal}). Weekend listening (particularly Saturday) shows slightly higher engagement than weekdays. These patterns motivated our temporal feature engineering.

\subsection{Temporal Consistency Issues}

We discovered that 0.45\% of records (${\approx}$34,000) exhibit pre-release listening---timestamps preceding the track's release date. Investigation revealed: 9.6\% were 1--7 days early (likely timezone differences), 70.3\% were 1 week to 1 month early (promotional access), 16.8\% were 1--3 months early (advance campaigns), and 2.7\% were over 1 year early (data entry errors or catalogue re-releases). The most extreme case was 17,081 days (47 years) before release. Rather than discarding these, we retained them and created a binary \texttt{is\_pre\_release\_listen} feature.

\subsection{Catalogue Analysis}

The track catalogue spans over a century (oldest release: 1912). We found that 55\% of listening involves deep catalogue tracks ($>$1 year old), while only 2\% are same-day listens of new releases. Track durations average 231 seconds (${\approx}$3.9 minutes) with a right-skewed distribution ranging from 1 to 2,822 seconds. These distributions informed our \texttt{track\_age\_category} and \texttt{duration\_category} features.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/temporal_patterns}
  \caption{Listen rate by hour of day (left) and day of week (right). Late-night listeners are more engaged; weekends show slightly higher listen rates.}
  \label{fig:temporal}
\end{figure}

%% ============================================================================
\section{Q1: How Would We Win This Competition?}
\label{sec:competition}

\subsection{Feature Engineering (47 Features)}
\label{sec:features}

We engineer features in seven categories, computing all statistics from training data only to prevent leakage.

\textbf{Temporal (9 features)}: From the listening timestamp, we extract hour (0--23), day of week (0--6), day of month, month, and binary flags for weekend, late night (1--5\,AM), evening (6--11\,PM), commute time (7--9\,AM), plus a categorical time-of-day variable. As our EDA showed, listening behaviour varies strongly with temporal context.

\textbf{Release (7 features)}: From the track release date: year, month, decade, days since release, pre-release listen flag, new release flag (within 30 days), and a track age category (pre-release/new/recent/catalogue/deep catalogue). New releases exhibit higher engagement, while deep catalogue tracks tend to be intentionally sought out.

\textbf{Duration (3 features)}: Duration in minutes, a categorical duration variable (very short to very long), and an extended track flag ($>$5 minutes). Track length correlates with skip probability.

\textbf{User engagement (9 features)}: Per-user statistics computed from training data only: listen rate, skip rate, session count, total listens, genre diversity (unique genres), artist diversity (unique artists), context variety, engagement segment, and a composite engagement score. These operationalise our EDA finding that user identity is the dominant predictor.

\textbf{Target encoding (3 features)}: High-cardinality identifiers (\texttt{genre\_id}, \texttt{artist\_id}, \texttt{album\_id}) are encoded using smoothed target encoding:
\begin{equation}
  \text{enc}(c) = \frac{n_c \cdot \bar{y}_c + m \cdot \bar{y}}{n_c + m}, \quad m = 50
\end{equation}
\textbf{Item-level (4 features)}: Smoothed listen rates and log-transformed play counts for both tracks (\texttt{media\_listen\_rate\_smooth}, \texttt{media\_play\_count\_log}) and artists (\texttt{artist\_listen\_rate\_smooth}, \texttt{artist\_play\_count\_log}). The log transformation addresses the heavy right-skew in play counts.

\textbf{User--item affinity (3 features)}: \texttt{user\_artist\_affinity} (smoothed user-specific artist listen rate), \texttt{user\_genre\_affinity} (user-specific genre listen rate), and \texttt{user\_knows\_artist} (binary prior-exposure flag). These approximate collaborative filtering within a feature-based framework. As our ablation study reveals (Section~\ref{sec:ablation}), these features are powerful but dangerously prone to overfitting.

\subsection{Models}

\textbf{Implicit ALS}~\cite{koren2009matrix}: We train a matrix factorisation model using Alternating Least Squares on the user--item interaction matrix (19,008 users $\times$ 429,642 items), treating listen counts as confidence weights. We use 64 latent factors with $\lambda = 0.1$ regularisation for 15 iterations. This represents the canonical collaborative filtering approach.

\textbf{XGBoost}~\cite{chen2016xgboost} and \textbf{LightGBM}~\cite{ke2017lightgbm}: Gradient-boosted trees with 500 estimators, max depth 7, learning rate 0.05, subsample 0.8, L1/L2 regularisation, early stopping (patience 30).

\textbf{Neural network}: Feedforward (256--128--64) with batch normalisation~\cite{ioffe2015batch}, dropout~\cite{srivastava2014dropout}, Adam~\cite{kingma2015adam}. Underperforms tree models by ${\approx}$1 pp on random splits, consistent with gradient boosting dominance on tabular data.

\textbf{Hybrid (XGBoost + MF embeddings)}: We extract 64-dimensional user and item embedding vectors from the trained ALS model and derive four features: dot product (primary CF signal), cosine similarity, user embedding norm, and item embedding norm. These are concatenated with the 47 engineered features, creating a 51-feature hybrid model that combines collaborative filtering with learning-to-rank.

\subsection{Evaluation: Random vs.\ Temporal Validation}
\label{sec:evaluation}

The choice of validation strategy proved to be the most consequential decision in our pipeline.

\subsubsection{Random Split (Na\"ive)}
A stratified 90/10 random split yields XGBoost AUC of \textbf{0.758} when user statistics are properly computed from only the training portion. (An earlier pipeline that computed user statistics from \emph{all} data before splitting inflated this to 0.935---a cautionary example of subtle data leakage.)

\subsubsection{Temporal Split (Realistic)}
Splitting chronologically---training on the first 90\% of interactions by timestamp and validating on the last 10\%---produces AUC of \textbf{0.681}. The temporal validation set covers the final 5 days (26--30 November 2016), with 85.5\% user overlap but 20.1\% unseen items. Crucially, the validation listen rate (0.642) is lower than training (0.689), reflecting real distributional shift. This is dramatically closer to the competition winner's \textbf{0.686}, validating temporal split as the appropriate evaluation methodology.

Figure~\ref{fig:pred_dist} shows the predicted probability distributions under random split evaluation, where the model achieves strong class separation. Under temporal evaluation, this separation degrades significantly, particularly for users encountering unfamiliar artists.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.85\linewidth]{figures/prediction_distribution}
  \caption{Distribution of predicted listen probabilities by actual outcome (random split, v1 model). Green: actual listens cluster near 1.0. Red: actual skips spread across lower values. This separation is weaker under temporal evaluation.}
  \label{fig:pred_dist}
\end{figure}

Table~\ref{tab:results} compares all models under temporal evaluation.

\begin{table}[t]
  \caption{Model performance under temporal split evaluation}
  \label{tab:results}
  \begin{tabular}{lr}
    \toprule
    Model & AUC \\
    \midrule
    Global mean baseline                  & 0.500 \\
    Implicit ALS (64 factors)             & 0.520 \\
    Per-item listen rate                  & 0.612 \\
    XGBoost (47 feat.)                    & 0.686 \\
    Hybrid: XGBoost + MF (51 feat.)       & 0.684 \\
    Hybrid: no affinity + MF (50 feat.)   & 0.722 \\
    Per-user listen rate                  & 0.728 \\
    XGBoost (no affinity, 44 feat.)       & 0.736 \\
    \textbf{Logistic Regression (47 feat.)}& \textbf{0.743} \\
    \midrule
    \textit{Competition winner}           & \textit{0.686} \\
    \bottomrule
  \end{tabular}
\end{table}

\subsubsection{Analysis of Results}

Several striking findings emerge from this comparison:

\textbf{ALS performs poorly (0.520).} Pure collaborative filtering barely exceeds random, despite being the canonical recommender systems technique. This makes sense: the task is not ``what should we recommend?'' but ``will this specific user skip this specific track in this specific context?''---a contextual prediction problem where latent factors capture preference but not intent.

\textbf{XGBoost underperforms logistic regression.} Despite being more expressive, XGBoost (0.686) scores below logistic regression (0.743). As the ablation below shows, XGBoost overfits to user--item affinity features that do not generalise forward in time, while logistic regression's linear constraints provide natural regularisation.

\textbf{MF embeddings do not help.} The hybrid model (0.684) performs comparably to base XGBoost (0.686). The ALS embeddings capture long-term preference patterns that are already approximated by our engineered affinity features---and suffer from the same temporal overfitting problem. Removing affinity features while keeping MF embeddings (0.722) confirms that the MF signal partially replaces affinity features but does not fully solve the generalisation problem.

\textbf{The simplest user-level baseline is remarkably strong.} Per-user listen rate alone (0.728) outperforms all complex models except logistic regression. This underscores our EDA finding that user identity is the dominant predictor of skip behaviour.

\subsection{Feature Ablation Study}
\label{sec:ablation}

To quantify which feature groups \emph{actually matter} for temporal generalisation, we removed each group and retrained XGBoost. Table~\ref{tab:ablation} shows the results, sorted by impact.

\begin{table}[t]
  \caption{Feature ablation under temporal split (full model AUC = 0.681)}
  \label{tab:ablation}
  \begin{tabular}{lrrr}
    \toprule
    Feature Group Removed & $n$ & AUC & $\Delta$ \\
    \midrule
    User--Item Affinity    & 3  & 0.732 & \textbf{+0.051} \\
    Temporal               & 9  & 0.692 & +0.011 \\
    User Engagement        & 9  & 0.691 & +0.009 \\
    Duration               & 3  & 0.686 & +0.005 \\
    Item-Level             & 4  & 0.685 & +0.004 \\
    Release                & 7  & 0.682 & +0.001 \\
    Target Encoding        & 3  & 0.682 & +0.001 \\
    \midrule
    \textit{None (full model)} & \textit{47} & \textit{0.681} & --- \\
    \bottomrule
  \end{tabular}
\end{table}

The most important finding: \textbf{removing user--item affinity features improves AUC by 5.1 percentage points} (from 0.681 to 0.732). These features---which dominate random-split importance rankings---overfit to historical user-artist interactions that do not predict future behaviour under distributional shift. The model learns to rely on ``this user liked this artist before'' signals that break when the test context involves new recommendations.

Similarly, removing user engagement and temporal features produces small improvements, suggesting the model overfits these signals under temporal evaluation. In contrast, item-level and target-encoded features have near-zero impact, indicating they neither help nor harm generalisation.

Figure~\ref{fig:importance} shows the v1 feature importance ranking, where user engagement features dominate. The ablation reveals this ranking is misleading under realistic evaluation.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.85\linewidth]{figures/feature_importance}
  \caption{Feature importances from XGBoost v1 (random split). User engagement features dominate, but ablation under temporal split reveals they do not generalise.}
  \label{fig:importance}
\end{figure}

\subsection{Lessons for Competition Strategy}

Our experiments reveal that maximising a random-split validation score is counterproductive. A more effective competition strategy would:
\begin{itemize}
  \item Use \textbf{temporal validation} from the start, accepting lower but realistic scores.
  \item \textbf{Regularise or remove} user--item affinity features that overfit to historical patterns.
  \item Focus on \textbf{robust features} (item popularity, logistic regression-style signals) that generalise across time.
  \item Invest in \textbf{sequence-aware modelling} (recent session context, genre transitions) rather than aggregated statistics.
\end{itemize}

%% ============================================================================
\section{Q2: What Would We Propose to Solve Deezer's Recommendation Problems?}
\label{sec:production}

If we worked at Deezer, we would not deploy our competition pipeline directly.

\subsection{System Architecture}

We propose a two-stage architecture common in industrial systems~\cite{covington2016deep}:

\begin{enumerate}
  \item \textbf{Candidate generation}: A lightweight model retrieves ${\sim}$1000 candidates from the full catalogue using approximate nearest neighbour search over learned user and item embeddings~\cite{he2017neural}. Our ALS model could serve this purpose---despite its weak standalone AUC, it captures broad preference patterns suitable for retrieval where recall matters more than precision.
  \item \textbf{Ranking}: A more expressive model (similar to our XGBoost pipeline but without overfitting-prone affinity features) scores candidates using contextual features. The top-$k$ ranked items are then passed through a diversity-aware re-ranking step.
\end{enumerate}

This decomposition enables sub-second latency even with catalogues of tens of millions of tracks.

\subsection{Cold Start}

The competition provides training data for every test user. In production, new users and new tracks arrive daily~\cite{schein2002methods}. Our experiments directly quantify this challenge:

\begin{itemize}
  \item Our temporal split had 20.1\% unseen items---and the model's AUC dropped from 0.758 (random) to 0.686 (temporal). Cold items directly degrade performance.
  \item Our ablation showed user--item affinity features are the most powerful but also most fragile. A cold-start user lacks exactly these signals, falling back to the global mean.
  \item Our ALS model's weak standalone performance (0.520) shows that even learned embeddings struggle with sparse cold-start interactions.
\end{itemize}

The production system must gracefully degrade: using demographic signals (our EDA showed age correlates with skip rate), content-based features (audio analysis, genre tags), and popularity-based recommendations for new users, with collaborative signals phased in as interaction history accumulates.

\subsection{Diversity and User Experience}

A system optimising only listen probability converges on popular, familiar tracks---the ``filter bubble''~\cite{mcnee2006being}. Flow's value proposition is specifically a mix of favourites and discoveries. Production requires diversity-aware re-ranking~\cite{castells2015novelty}, familiarity-novelty balance calibrated per user, and fair exposure for niche artists.

\subsection{Scalability and Adaptation}

User statistics and item features must update incrementally. Our temporal split showed that the validation listen rate (0.642) differs from training (0.689)---distribution drift that production systems must detect and adapt to. Session-aware models~\cite{covington2016deep} would capture sequential patterns that our pointwise classifier misses.

%% ============================================================================
\section{Q3: Do These Two Solutions Overlap?}
\label{sec:comparison}

\subsection{Where They Overlap}

Several aspects of our work transfer directly:
\begin{itemize}
  \item \textbf{Feature engineering}: Temporal context and item popularity proved valuable and robust under temporal evaluation, making them reliable production features.
  \item \textbf{Leakage-free design}: Our strict separation of training-time statistics mirrors the production requirement that models must never use future information.
  \item \textbf{Model families}: XGBoost and LightGBM are widely deployed in production ranking systems at companies like Airbnb and Microsoft~\cite{chen2016xgboost, ke2017lightgbm}, offering sub-millisecond inference.
  \item \textbf{Two-stage architecture}: Our ALS model could serve as candidate generator while XGBoost (without affinity features) serves as ranker---a natural production decomposition.
\end{itemize}

\subsection{Where They Diverge}

\begin{table}[t]
  \caption{Competition vs.\ production: fundamental differences}
  \label{tab:tradeoffs}
  \begin{tabular}{p{2.4cm}p{4.5cm}p{4.5cm}}
    \toprule
    Dimension & Competition & Production \\
    \midrule
    Objective & Maximise AUC on fixed test set & Long-term retention and satisfaction \\
    Cold start & All test users in training & New users/tracks arrive daily \\
    Diversity & Irrelevant to AUC & Essential for Flow's promise \\
    Latency & Batch, unlimited & $<$100\,ms per request \\
    Feedback & Static dataset & Predictions influence future data \\
    \bottomrule
  \end{tabular}
\end{table}

\subsubsection{Our Ablation Exposes the Core Tension}

Our most striking finding directly illustrates the competition--production gap. User--item affinity features \emph{dominate} random-split evaluation (accounting for $>$70\% of feature importance) but \emph{actively harm} temporal generalisation (removing them improves AUC by 5.1 pp). In production, this pattern would be catastrophic: the model would learn to recommend artists users already know, creating a feedback loop that suppresses discovery. The competition incentivises exactly the features that would fail in production.

\subsubsection{Competition Tricks That Hurt in Production}

Target encoding creates popularity feedback loops. Complex ensembles add marginal AUC but double maintenance cost. Overfitting to historical user-artist patterns---our ablation's key finding---would narrow recommendations and degrade long-term engagement.

\subsubsection{Production Requirements Competitions Ignore}

Latency constraints (Flow must respond in real time), explainability (``Because you liked Artist X''), and artist ecosystem fairness (balancing user satisfaction with fair exposure) are entirely absent from competition evaluation.

\subsection{Synthesis}

The competition pipeline provides a ranking component, but cannot function standalone. Production requires candidate generation, cold-start fallbacks, diversity mechanisms, and continuous monitoring. Our ablation demonstrates the deeper point: \emph{the features that maximise offline metrics are precisely those that fail under distributional shift}---and production \emph{is} distributional shift.

%% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

We developed a comprehensive approach to Deezer skip prediction, comparing feature-based classifiers, implicit matrix factorisation, and a hybrid model. Our key findings are methodological rather than architectural: evaluation strategy matters more than model complexity. A temporal split produces 0.686 AUC---matching the competition winner---while a random split inflates this to 0.758. Our ablation revealed that user--item affinity features degrade temporal performance by 5.1 pp, and pure ALS achieves only 0.520, demonstrating that skip prediction is a contextual classification problem where collaborative filtering signals are necessary but insufficient.

These findings carry implications across paradigms. For competition participants, they argue for temporal validation from the outset. For recommender systems practitioners, they show that the features which dominate offline evaluation may create the most harmful feedback loops in production. For the field, they demonstrate that the boundary between ``recommendation'' and ``classification'' is blurred: effective skip prediction requires elements of both, but the relative contribution of each depends critically on the evaluation regime.

%% ============================================================================

\section*{Declaration on Generative AI}
During the preparation of this work, the authors used AI-assisted tools for code development and report drafting. The authors reviewed and edited all content and take full responsibility for the publication's content.

%% ============================================================================

\section*{Author Contributions}

\begin{itemize}
  \item \textbf{Vimerlin Govender}: Data preprocessing pipeline, XGBoost/LightGBM experiments, temporal validation and ablation analysis, report writing.
  \item \textbf{Meleknur \"Ozg\"u}: Exploratory data analysis, user segmentation, collaborative filtering baselines, production system design discussion.
  \item \textbf{Xinmeng Song}: Feature engineering (temporal, release, duration features), neural network experiments, baseline model comparison, code review.
\end{itemize}

%% ============================================================================
\bibliography{references}

\end{document}
