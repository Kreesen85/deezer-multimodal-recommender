\documentclass[]{ceurart}

\sloppy

\usepackage{listings}
\lstset{breaklines=true}
\usepackage{booktabs}
\usepackage{amsmath}

\begin{document}

\copyrightyear{2026}
\copyrightclause{Copyright for this paper by its authors.
  Use permitted under Creative Commons License Attribution 4.0
  International (CC BY 4.0).}

\conference{Recommender Systems Course Project, February 2026}

\title{Predicting Music Listening Behaviour on Deezer: From Competition Pipelines to Production Recommender Systems}

\author[1]{Vimerlin Govender}[%
email=vimerlin.govender@stud.hslu.ch,
]
\author[1]{Meleknur \"Ozg\"u}[%
email=meleknur.oezgue@stud.hslu.ch,
]
\author[1]{Xinmeng Song}[%
email=xinmeng.song@stud.hslu.ch,
]
\address[1]{Lucerne University of Applied Sciences and Arts (HSLU)}

%%
\begin{abstract}
We address the Deezer skip prediction task from the 2017 Data Science Game, where the goal is to predict whether a user will listen to or skip the first track recommended by Deezer's Flow feature. We frame this as a binary classification problem evaluated by ROC AUC. Our competition-focused pipeline engineers 47 features spanning temporal context, release metadata, user engagement statistics, target-encoded categorical variables, item popularity, and user-item affinity signals. An ensemble of XGBoost and LightGBM achieves a cross-validation AUC of 0.935 on a random training split, though we critically examine why this substantially exceeds the winning competition score of 0.686, identifying evaluation methodology as the key differentiator. We then contrast this competition solution with a production-grade recommendation architecture for Deezer, and analyse where competition-driven and production-driven approaches overlap and diverge.
\end{abstract}

\begin{keywords}
  recommender systems \sep
  music recommendation \sep
  skip prediction \sep
  gradient boosting \sep
  feature engineering
\end{keywords}

\maketitle

%% ============================================================================
\section{Introduction}
\label{sec:intro}

Music streaming platforms such as Deezer serve hundreds of millions of tracks to users with diverse tastes and listening contexts. Deezer's \emph{Flow} feature provides a personalised, infinite stream of music---a mix of familiar favourites and new discoveries. Predicting whether a user will engage with (listen to) or reject (skip) a Flow recommendation is central to improving this experience~\cite{schedl2018current}.

The Deezer Data Science Game 2017 (DSG17) formalises this as a binary classification task: given a user's one-month listening history, predict whether they will listen to or skip the first track Flow recommends~\cite{deezer2017dsg}. A track is considered ``listened'' if the user hears more than 30 seconds of it (\texttt{is\_listened}~=~1); pressing skip before 30 seconds yields \texttt{is\_listened}~=~0. The competition attracted strong international participation, with the winning team (``lebed i 3 raka'', MSU Russia) achieving a test AUC of 0.686.

In this report, we pursue three objectives aligned with the course requirements. First, we develop a competition-focused solution that maximises AUC through feature engineering and model ensembling (\textbf{Q1}, Section~\ref{sec:competition}). Second, we propose a production-grade recommendation system for Deezer's broader needs (\textbf{Q2}, Section~\ref{sec:production}). Third, we critically compare these two perspectives (\textbf{Q3}, Section~\ref{sec:comparison}).

%% ============================================================================
\section{Dataset and Problem Setup}
\label{sec:data}

\subsection{Dataset Description}

The training set contains 7,558,834 interactions from 19,914 users across 451,867 tracks, representing one month of listening history. Each record includes user and item identifiers (\texttt{user\_id}, \texttt{media\_id}, \texttt{artist\_id}, \texttt{album\_id}, \texttt{genre\_id}), a Unix timestamp, track release date, song duration, and the binary target \texttt{is\_listened}. The overall listen rate is 68.4\%.

The test set contains exactly 19,918 rows---approximately one row per user---each representing the \emph{first track Flow recommended} to that user. This is a fundamentally different distribution from training: the training data captures general listening behaviour across many tracks per user, while the test set captures a single, curated recommendation event per user.

\subsection{Problem Formulation}

Given a user $u$, a track $t$, and contextual features $\mathbf{c}$, we predict $P(\text{is\_listened} = 1 \mid u, t, \mathbf{c})$. Predictions are evaluated using ROC AUC. The challenge lies in generalising from broad listening history (training) to the specific context of a Flow recommendation (test).

%% ============================================================================
\section{Q1: How Would We Win This Competition?}
\label{sec:competition}

If our sole objective were to maximise the Kaggle leaderboard score, our strategy would centre on three pillars: (1)~comprehensive feature engineering, (2)~strong gradient-boosted tree models, and (3)~rigorous validation that mirrors the test distribution.

\subsection{Feature Engineering}
\label{sec:features}

We engineer 47 features in seven categories. All user-level and interaction-level statistics are computed exclusively from training data to prevent data leakage.

\subsubsection{Temporal Features (9 features)}
From the listening timestamp: \texttt{hour}, \texttt{day\_of\_week}, \texttt{day\_of\_month}, \texttt{month}, \texttt{is\_weekend}, \texttt{is\_late\_night} (1--5\,AM), \texttt{is\_evening} (6--11\,PM), \texttt{is\_commute\_time} (7--9\,AM), and \texttt{time\_of\_day} (categorical). Listening behaviour varies strongly with temporal context---users are more selective during commutes than evening relaxation.

\subsubsection{Release Features (7 features)}
From the track release date: \texttt{release\_year}, \texttt{release\_month}, \texttt{release\_decade}, \texttt{days\_since\_release}, \texttt{is\_pre\_release\_listen}, \texttt{is\_new\_release} (within 30 days), and \texttt{track\_age\_category}. New releases exhibit higher engagement, while deep catalogue tracks tend to be intentionally sought out.

\subsubsection{Duration Features (3 features)}
\texttt{duration\_minutes}, categorical \texttt{duration\_category}, and \texttt{is\_extended\_track} ($>$5 minutes). Track length correlates with skip probability.

\subsubsection{User Engagement Features (9 features)}
Per-user statistics from training data: \texttt{user\_listen\_rate}, \texttt{user\_skip\_rate}, \texttt{user\_session\_count}, \texttt{user\_total\_listens}, \texttt{user\_genre\_diversity}, \texttt{user\_artist\_diversity}, \texttt{user\_context\_variety}, \texttt{user\_engagement\_segment}, and \texttt{user\_engagement\_score}. These capture the insight that some users are inherently more selective than others.

\subsubsection{Target-Encoded Categoricals (3 features)}
High-cardinality identifiers (\texttt{genre\_id}, \texttt{artist\_id}, \texttt{album\_id}) are encoded using smoothed target encoding:
\begin{equation}
  \text{enc}(c) = \frac{n_c \cdot \bar{y}_c + m \cdot \bar{y}}{n_c + m}
\end{equation}
where $n_c$ is the category count, $\bar{y}_c$ the category mean, $\bar{y}$ the global mean, and $m=50$ the smoothing parameter. This converts identifiers into informative numerical features while regularising rare categories toward the global mean.

\subsubsection{Item-Level Features (4 features)}
Smoothed listen rates and log-transformed play counts for tracks (\texttt{media\_listen\_rate\_smooth}, \texttt{media\_play\_count\_log}) and artists (\texttt{artist\_listen\_rate\_smooth}, \texttt{artist\_play\_count\_log}).

\subsubsection{User--Item Affinity Features (3 features)}
\texttt{user\_artist\_affinity} (smoothed user-specific artist listen rate), \texttt{user\_genre\_affinity} (user-specific genre listen rate), and \texttt{user\_knows\_artist} (binary prior exposure flag). These approximate collaborative filtering within a feature-based framework.

\subsection{Model Selection}

\subsubsection{XGBoost and LightGBM}
Our primary models are XGBoost~\cite{chen2016xgboost} and LightGBM~\cite{ke2017lightgbm}, both gradient-boosted decision tree frameworks. Both use 500 estimators, max depth 7, learning rate 0.05, subsample 0.8, and L1/L2 regularisation. Early stopping with patience 30 prevents overfitting.

\subsubsection{Neural Network}
We also train a feedforward neural network (256--128--64 units) with batch normalisation~\cite{ioffe2015batch}, dropout~\cite{srivastava2014dropout}, and Adam optimiser~\cite{kingma2015adam}. Despite being trained on the same 47 features, it underperforms tree-based models, consistent with findings that gradient boosting dominates on tabular data.

\subsubsection{Ensemble}
We combine XGBoost and LightGBM via weighted averaging: $\hat{p} = w \cdot \hat{p}_{\text{XGB}} + (1-w) \cdot \hat{p}_{\text{LGB}}$, with $w$ optimised by grid search on the validation set.

\subsection{Results and Comparison with Competition Winners}

Table~\ref{tab:results} summarises our model progression.

\begin{table}[t]
  \caption{Model performance on a stratified 90/10 random split of the training data}
  \label{tab:results}
  \begin{tabular}{lccr}
    \toprule
    Model & Features & Data Size & Val AUC \\
    \midrule
    XGBoost v1 (baseline)  & 35 & 100K   & 0.872 \\
    XGBoost v2             & 47 & 7.5M   & 0.934 \\
    LightGBM v2            & 47 & 7.5M   & 0.935 \\
    Ensemble (XGB + LGB)   & 47 & 7.5M   & 0.935 \\
    Neural Network         & 47 & 7.5M   & 0.926 \\
    \midrule
    \textit{Competition winner (test)} & \textit{?} & \textit{7.5M} & \textit{0.686} \\
    \bottomrule
  \end{tabular}
\end{table}

\subsubsection{Why Our Validation AUC Far Exceeds the Competition Scores}

Our validation AUC of 0.935 dramatically exceeds the winning test score of 0.686. This discrepancy is \emph{not} because our model is superior---it reflects a fundamental difference in evaluation methodology:

\begin{enumerate}
  \item \textbf{Distribution mismatch}: Our validation set is a random sample from the same training distribution (general listening history). The competition test set consists of \emph{first Flow recommendations}---a curated, algorithmically selected context that differs from organic listening behaviour.
  \item \textbf{Inflated validation from user overlap}: In our random split, the same users appear in both train and validation sets with many interactions each, allowing user-level features (listen rate, affinity scores) to be highly predictive. On the actual test set, the model must predict a \emph{single} new context per user.
  \item \textbf{Target leakage through item features}: Our item-level features (media listen rate, artist listen rate) are computed from the full training set. In a random split, validation items likely appear in training. On the held-out test set, items may be entirely new to the model.
\end{enumerate}

\subsubsection{What Competition Winners Likely Did Differently}

The top teams (AUC 0.68--0.69) likely employed strategies we did not:
\begin{itemize}
  \item \textbf{Temporal validation}: Splitting by time rather than randomly, since the test set represents future events. This gives a more realistic estimate of generalisation.
  \item \textbf{Sequence-aware models}: Modelling the \emph{order} of listening events, not just aggregated statistics, since Flow's first recommendation depends on recent session context.
  \item \textbf{Context-specific features}: Engineering features that capture \emph{why Flow selected this track}---e.g., novelty relative to the user's history, genre transition patterns, and session-level momentum.
  \item \textbf{Robust regularisation}: Avoiding over-reliance on user-item statistics that inflate validation but fail on distribution-shifted test data.
\end{itemize}

This analysis highlights a critical lesson: \textbf{a high validation score on a convenient split does not imply competition success}. Winning requires evaluation methodology that mirrors the test distribution.

%% ============================================================================
\section{Q2: What Would We Propose to Solve Deezer's Recommendation Problems?}
\label{sec:production}

If we worked at Deezer, we would not deploy our competition pipeline directly. A production recommendation system must address challenges that a Kaggle competition ignores entirely.

\subsection{System Architecture}

We propose a two-stage architecture common in industrial recommender systems~\cite{covington2016deep}:

\begin{enumerate}
  \item \textbf{Candidate generation}: A lightweight retrieval model selects ${\sim}$1000 candidate tracks from the full catalogue using approximate nearest neighbour search over learned user and item embeddings~\cite{he2017neural}. This stage prioritises recall.
  \item \textbf{Ranking}: A more expressive model scores each candidate using rich features---similar to our competition features but augmented with real-time session context. The top-$k$ items are presented after a diversity-aware re-ranking step.
\end{enumerate}

This decomposition enables sub-second latency even with catalogues of tens of millions of tracks.

\subsection{Cold-Start Problem}
\label{sec:coldstart}

The competition conveniently provides training data for every test user. In production, new users and new tracks arrive constantly~\cite{schein2002methods}:

\begin{itemize}
  \item \textbf{New users}: Without listening history, we rely on onboarding preferences (selected genres/artists), demographic signals, and popularity-based recommendations. As interactions accumulate, collaborative signals gradually take over.
  \item \textbf{New tracks}: For tracks with zero play history, we use content-based features extracted from audio (tempo, energy, valence), artist metadata, and editorial tags. Our smoothed target encoding naturally handles this by falling back to the global mean when $n_c = 0$.
  \item \textbf{Exploration}: An $\epsilon$-greedy strategy occasionally surfaces new or less popular tracks, accelerating data collection for cold items while preventing the system from converging on a narrow set of safe recommendations.
\end{itemize}

\subsection{Diversity and User Experience}

A system that only optimises listen probability will converge on a narrow set of popular, familiar tracks---the ``filter bubble''~\cite{mcnee2006being}. Flow's value proposition is specifically a \emph{mix of favourites and discoveries}. A production system must therefore:

\begin{itemize}
  \item \textbf{Enforce diversity}: Re-rank to ensure variety in artist, genre, tempo, and mood within each session~\cite{castells2015novelty}.
  \item \textbf{Balance familiarity and novelty}: Surface known artists alongside new discoveries, calibrated to each user's openness to exploration.
  \item \textbf{Ensure fairness}: Avoid systematic bias against niche artists or less popular genres, which harms both user experience and the artist ecosystem.
\end{itemize}

\subsection{Scalability and Long-Term Engagement}

\begin{itemize}
  \item \textbf{Incremental updates}: User statistics and item features must update incrementally as new interactions arrive, not require full recomputation.
  \item \textbf{Real-time serving}: Tree-based models serve predictions in $<$1\,ms; embedding-based retrieval uses approximate nearest neighbour libraries (e.g., FAISS) for sub-linear catalogue search.
  \item \textbf{User evolution}: Musical tastes change over time. Models should weight recent interactions more heavily and adapt to evolving preferences rather than anchoring on historical averages.
  \item \textbf{Session awareness}: Users listen in sessions with internal coherence (workout music vs.\ study music). Context-aware and session-based models~\cite{covington2016deep} capture these patterns that pointwise classifiers miss.
\end{itemize}

%% ============================================================================
\section{Q3: Do These Two Solutions Overlap? Why or Why Not?}
\label{sec:comparison}

The competition solution and production system share some foundations but diverge in fundamental ways.

\subsection{Where They Overlap}

\begin{itemize}
  \item \textbf{Feature engineering transfers}: Temporal context, user engagement statistics, and item popularity are valuable in both settings. The insight that user-artist affinity is the strongest predictor applies universally.
  \item \textbf{Leakage-free design}: Our strict separation of training and evaluation statistics mirrors the production requirement that models must never use future information.
  \item \textbf{Scalable model families}: XGBoost and LightGBM are widely used in production ranking systems at companies like Airbnb and Microsoft due to their speed, interpretability, and robustness~\cite{chen2016xgboost, ke2017lightgbm}.
  \item \textbf{The ranking stage}: Our competition pipeline could serve directly as the second-stage ranker in the production architecture, scoring pre-selected candidates with rich features.
\end{itemize}

\subsection{Where They Diverge}

\begin{table}[t]
  \caption{Competition vs.\ production: fundamental differences}
  \label{tab:tradeoffs}
  \begin{tabular}{p{2.6cm}p{4.5cm}p{4.5cm}}
    \toprule
    Dimension & Competition & Production \\
    \midrule
    Objective & Maximise AUC on fixed test set & Maximise long-term user retention and satisfaction \\
    Cold start & Not a concern (all test users in training) & Critical---new users and tracks arrive daily \\
    Diversity & Irrelevant to AUC & Essential for Flow's ``favourites + discoveries'' promise \\
    Latency & Batch prediction, unlimited time & Must be $<$100\,ms per request \\
    Fairness & Not evaluated & Legal and ethical requirement \\
    Feedback loop & Static dataset & Dynamic: predictions influence future user behaviour \\
    Evaluation & Single offline metric & A/B tests, retention, session length, user surveys \\
    \bottomrule
  \end{tabular}
\end{table}

\subsubsection{Accuracy Is Necessary but Not Sufficient}

The most important divergence is in \emph{what success means}. In the competition, a model that perfectly separates listens from skips wins---regardless of whether it recommends the same ten songs to every user. In production, such a model would destroy user engagement within weeks~\cite{mcnee2006being}. Real recommendation quality requires balancing accuracy, diversity, novelty, and fairness---none of which AUC captures.

\subsubsection{Competition Tricks That Hurt in Production}

Several techniques that boost competition scores are counterproductive in production:

\begin{itemize}
  \item \textbf{Heavy target encoding}: Our target-encoded features are powerful but create feedback loops in production---popular items get higher encoded scores, receive more recommendations, and become even more popular, starving niche content.
  \item \textbf{Complex ensembles}: Our XGBoost + LightGBM ensemble adds marginal AUC improvement but doubles model maintenance, deployment complexity, and debugging difficulty. In production, a single well-tuned model is usually preferred.
  \item \textbf{Overfitting to distribution}: Competition models overfit to the specific train/test split. Production data is non-stationary---user tastes shift, new music releases change the catalogue, and seasonal patterns create distribution drift.
\end{itemize}

\subsubsection{Production Requirements That Competitions Ignore}

Conversely, several production-critical concerns are entirely absent from competitions:

\begin{itemize}
  \item \textbf{Latency constraints}: Flow must generate recommendations in real time as users press play. Our batch pipeline has no such constraint.
  \item \textbf{Explainability}: When Flow recommends a track, users benefit from understanding why (``Because you liked Artist X''). Tree models offer feature importance, but not per-prediction explanations suitable for users.
  \item \textbf{Artist ecosystem}: Deezer must balance user satisfaction with fair exposure for artists and labels---a multi-stakeholder optimisation that no competition metric captures.
\end{itemize}

\subsection{Synthesis}

The competition pipeline provides a strong \emph{component} for production---specifically, the ranking model that scores candidate tracks. However, it cannot function as a standalone system. Production deployment requires wrapping it in a candidate generation layer, diversity-aware re-ranking, cold-start fallbacks, and continuous monitoring infrastructure. The core lesson is that \emph{winning a Kaggle competition and building a good recommender system are related but distinct objectives}, and understanding this distinction is essential for practitioners.

%% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

We presented a comprehensive approach to the Deezer skip prediction challenge, developing a 47-feature pipeline with an XGBoost/LightGBM ensemble that achieves 0.935 AUC on a random validation split. We critically examined why this far exceeds the competition winner's 0.686, attributing the gap to evaluation methodology: our random split inflates user-item feature utility, while the actual test set requires generalisation to a distribution-shifted recommendation context.

We contrasted this competition solution with a production architecture addressing cold start, scalability, diversity, and long-term engagement. Our comparison shows that competition solutions provide excellent ranking components but require fundamental architectural augmentation for production deployment. The strongest teams would recognise that \emph{building a system users love requires more than maximising a single offline metric}.

%% ============================================================================

\section*{Declaration on Generative AI}
During the preparation of this work, the authors used AI-assisted tools for code development and report drafting. The authors reviewed and edited all content and take full responsibility for the publication's content.

%% ============================================================================

\section*{Author Contributions}

\begin{itemize}
  \item \textbf{Vimerlin Govender}: [TODO: Describe contributions]
  \item \textbf{Meleknur \"Ozg\"u}: [TODO: Describe contributions]
  \item \textbf{Xinmeng Song}: [TODO: Describe contributions]
\end{itemize}

%% ============================================================================
\bibliography{references}

\end{document}
