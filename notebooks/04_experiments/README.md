# 04_experiments - Model Experiments

This folder contains experimental models and approaches for the Deezer Skip Prediction challenge.

---

## ğŸ¯ Task Overview

**Objective:** Predict whether users will listen to their recommended tracks (>30 seconds) or skip them

**Evaluation Metric:** ROC AUC

**Dataset:**
- Training: User listening history over 1 month (~7.5M interactions)
- Test: First recommended track for each user (one row per user)

---

## ğŸ“‚ Experiment Structure

```
04_experiments/
â”œâ”€â”€ xgboost/                    # XGBoost baseline (CURRENT)
â”‚   â”œâ”€â”€ xgboost_baseline.py    # Training script
â”‚   â”œâ”€â”€ README.md              # Detailed documentation
â”‚   â””â”€â”€ run.sh                 # Quick start script
â”‚
â”œâ”€â”€ lightgbm/                   # LightGBM experiments (TODO)
â”œâ”€â”€ neural_net/                 # Deep learning models (TODO)
â”œâ”€â”€ collaborative_filtering/    # CF-based approaches (TODO)
â””â”€â”€ ensemble/                   # Model ensembles (TODO)
```

---

## ğŸš€ Current Experiments

### âœ… 1. XGBoost Baseline

**Status:** Implemented  
**Location:** `xgboost/`

**Features:**
- Uses all 46 engineered features (temporal, release, duration, user engagement)
- ROC AUC evaluation on validation set
- Feature importance analysis
- Comprehensive visualizations

**Quick Start:**
```bash
cd xgboost
./run.sh
```

**Expected Performance:** 0.70-0.75 ROC AUC

**Documentation:** See `xgboost/README.md`

---

## ğŸ“‹ Planned Experiments

### ğŸ”² 2. LightGBM
- Faster training than XGBoost
- Better handling of categorical features
- Expected: 0.71-0.76 ROC AUC

### ğŸ”² 3. Collaborative Filtering + Features
- Hybrid approach: CF embeddings + engineered features
- User/item latent factors from SVD/NMF
- Expected: 0.72-0.76 ROC AUC

### ğŸ”² 4. Neural Networks
- Deep feedforward network
- Learn non-linear feature interactions
- Expected: 0.69-0.74 ROC AUC

### ğŸ”² 5. Sequential Models (LSTM/GRU)
- Use user's listening sequence
- Capture temporal patterns
- Expected: 0.71-0.75 ROC AUC

### ğŸ”² 6. Model Ensemble
- Weighted combination of best models
- Stacking/blending approaches
- Expected: 0.76-0.80 ROC AUC

---

## ğŸ“Š Experiment Tracking

| Experiment | Status | ROC AUC (Val) | Features | Notes |
|------------|--------|---------------|----------|-------|
| XGBoost Baseline | âœ… Done | TBD | 46 | Baseline model |
| LightGBM | ğŸ”² TODO | - | 46 | - |
| XGB + CF | ğŸ”² TODO | - | 46 + CF | - |
| Neural Net | ğŸ”² TODO | - | 46 | - |
| Ensemble | ğŸ”² TODO | - | All | - |

---

## ğŸ”§ Best Practices

### Experiment Workflow:
1. **Create folder** for each experiment (e.g., `xgboost/`)
2. **Write script** with clear documentation
3. **Add README** with usage instructions
4. **Save outputs**: model, predictions, metrics, plots
5. **Gitignore outputs** (keep only code and docs)
6. **Update tracking table** above with results

### Output Files (per experiment):
- `model.*` - Trained model
- `feature_importance.csv` - Feature analysis
- `validation_predictions.csv` - Predictions for error analysis
- `metrics_summary.json` - Performance metrics
- `*.png` - Visualizations

---

## ğŸ¯ Performance Goals

| Phase | Target ROC AUC | Approaches |
|-------|----------------|------------|
| Phase 1: Baseline | 0.70-0.72 | Single feature-based model |
| Phase 2: Improved | 0.72-0.75 | Better features, tuning, CF hybrid |
| Phase 3: Advanced | 0.75-0.78 | Ensemble, sequential models |
| Phase 4: Optimized | 0.78-0.80 | Full ensemble, feature engineering |

---

## ğŸ“š References

- **XGBoost**: https://xgboost.readthedocs.io/
- **LightGBM**: https://lightgbm.readthedocs.io/
- **Scikit-learn**: https://scikit-learn.org/
- **TensorFlow/Keras**: https://www.tensorflow.org/
- **ROC AUC**: https://en.wikipedia.org/wiki/Receiver_operating_characteristic

---

## ğŸ› Troubleshooting

### Missing preprocessed data?
```bash
cd ../02_preprocessing
python demo_preprocessing_with_users.py
```

### Missing packages?
```bash
pip install xgboost lightgbm scikit-learn matplotlib seaborn
```

---

**Last Updated:** 2026-02-01  
**Current Focus:** XGBoost Baseline
